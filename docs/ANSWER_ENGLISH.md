
# Kapilan's MoovitaMix Data Pipeline Setup and Usage Guide

## Usage of the Solution

Setup and Usage Video: https://www.loom.com/share/629a712583404547afdae7979f30bdbb?sid=c804f58d-0510-4190-81f8-201864953e52

### Dependency

- Ensure you have **Python 3.7 or higher** installed on your system. 

### Setting Up The Environment

1.  **Install `virtualenv`** if you haven't already:

    > pip install virtualenv

2.  **Create a new virtual environment**:

    > virtualenv venv

3.  **Activate the virtual environment**:

- On Windows:


    > venv\Scripts\activate


- On macOS and Linux:


    > source venv/bin/activate


4.  **Navigate to the root directory of the project and install the package**: 

    > pip install . 

### Setting Environment Variables 

Before running the pipeline, you can set the following environment variables to customize the behavior:

-  `BASE_URL`: The base URL for the server (default is a predefined `DEFAULT_BASE_URL`)

-  `SCHEDULED_TIME`: The time at which the pipeline should run daily (default is a predefined `DEFAULT_SCHEDULED_TIME`)

-  `DATABASE_NAME`: The name of the database file (default is a predefined `DEFAULT_DATABASE_NAME`)

You can set these variables in your shell or create a `.env` file in the src/kap_moovita_mix_pipeline directory with the following content:

```
BASE_URL=your_custom_base_url
SCHEDULED_TIME=your_custom_time
DATABASE_NAME=your_custom_database_name
```

### Running the Application

   1.  **Start the server**:

        Open a terminal and run:

        > start-server

   2.  **Run the pipeline once**:

        Open another terminal and run:

        > start-pipeline --test

   3.  **Run the pipeline when scheduled**:

        Open another terminal and run:

        > start-pipeline

   4.  **Run the Tests**:

        Open another terminal and run:

        > run-tests

### Additional Commands

- To see the help screen for the pipeline:

    > start-pipeline -h


### Notes

- The `start-pipeline` command will by default wait until the scheduled time to run. Use the `--test` flag to bypass this wait and run immediately.

- Make sure both the server and the pipeline are running for the full functionality of the application.

- The `run-tests` command will execute all the pytest tests in the project.

<br />

# Questions (steps 4 to 7)

## Database Design and Application

The schema we're using comes from the API documentation generated by FastAPI. This is an excellent feature of FastAPI, as it automatically generates documentation based on our code structure and type annotations.
```
CREATE_SONGS_TABLE = '''
CREATE TABLE IF NOT EXISTS songs (
    id INTEGER PRIMARY KEY,
    name TEXT,
    artist TEXT,
    songwriters TEXT,
    duration TEXT,
    genres TEXT,
    album TEXT,
    created_at TEXT,
    updated_at TEXT
)
'''

CREATE_USERS_TABLE = '''
CREATE TABLE IF NOT EXISTS users (
    id INTEGER PRIMARY KEY,
    first_name TEXT,
    last_name TEXT,
    email TEXT,
    gender TEXT,
    favorite_genres TEXT,
    created_at TEXT,
    updated_at TEXT
)
'''

CREATE_LISTENING_HISTORY_TABLE = '''
CREATE TABLE IF NOT EXISTS listening_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER,
    items TEXT,
    created_at TEXT,
    updated_at TEXT,
    FOREIGN KEY (user_id) REFERENCES users(id)
)
'''
```

Currently, we're using SQLite as our output database file. Here are some advantages and disadvantages I've observed regarding the use of SQLite in this context:

### Advantages of using SQLite:

1. **Simplicity**: SQLite doesn't require a separate server process, which aligns perfectly with FastAPI's goal of being simple and easy to use.

2. **Built-in support**: Python has built-in support for SQLite, making it a natural choice for our FastAPI project.

3. **Portability**: As the entire database is contained in a single file, it's easy to move or back up, which is very convenient during development and testing.

4. **Quick setup**: I can get started quickly without complex database configuration, which is ideal for rapid prototyping.

5. **Suitable for testing**: SQLite's in-memory database functionality is excellent for running tests, which is crucial for our FastAPI application.

### Disadvantages of using SQLite:

1. **Concurrency limitations**: One drawback is that SQLite has limitations on concurrent writes, which could become problematic if our application scales and handles many simultaneous users.

2. **Limited SQL feature set**: While it works for many applications, SQLite lacks some advanced features we might need as the project grows.

3. **Network access**: SQLite doesn't support client/server operations over a network, which could limit our deployment options in the future.

4. **Scalability concerns**: For larger datasets or high-traffic applications, I'm not sure SQLite will hold up as well as other options.

Given that this schema comes from FastAPI documentation, it makes sense for us to use SQLite for now, especially during the initial development phase. It's perfect for small applications and rapid development. However, if we anticipate significant growth or need more advanced database features in the future, I'll certainly consider switching to a more robust system like PostgreSQL while continuing to take advantage of FastAPI's excellent schema generation and documentation capabilities.

## Monitoring System for the Pipeline

I would set up a detailed logging system within the `daily_data_retrieval` function. 

This logging would allow me to track each step of the process, including its start and end, the data retrieved, and any issues encountered.

In addition to logging, I would implement a notification system - such as email alerts or messages via a service like Slack - to inform us of any critical errors or if the pipeline isn't running as expected.

### For monitoring, I would focus on several key metrics:

1. **Execution Status**: I would track whether each daily run succeeded or failed.

2. **Execution Time**: Measuring how long the pipeline takes to run would help identify performance bottlenecks.

3. **Data Volume**: I would keep an eye on the number of records processed for each type of data - songs, users, and listening history.

4. **Error Rate**: Tracking the number and types of errors encountered during execution would be crucial for maintaining reliability.

5. **Data Latency**: I would want to measure the delay between data collection and its availability in the database.

6. **Resource Utilization**: Monitoring CPU and memory usage during execution would help ensure our application is running efficiently.

To implement this, I would modify the existing code to include logging statements at critical points in the `daily_data_retrieval` function. 

This way, I could capture important metrics and record them for later analysis.

Overall, this approach would allow me to create a more robust monitoring system for our data pipeline, enabling us to react quickly to any issues and maintain smooth operations.

## Personalization of Music Recommendations

To personalize our recommendations and align them with each user's listening habits, I will implement a cluster-based approach.

I will use the K-means algorithm to group songs into clusters based on their audio characteristics and metadata. Then, I will analyze each user's listening history to identify the most frequently listened clusters.

When generating recommendations, I will prioritize songs from these preferred clusters, ensuring that the next recommended song is more likely to come from a cluster for which the user has shown a strong affinity.

This method will allow me to maintain relevance while offering variety within the user's preferred styles. I will implement a weighted random selection process, where clusters will be chosen with probabilities proportional to the user's historical engagement with them.

This approach will balance familiarity and discovery, offering users recommendations that are both comfortable and novel. By tailoring recommendations to each user's cluster preferences, I believe we will enhance the personalization of the music discovery experience, potentially increasing user satisfaction and engagement with our platform.

## Automation of the Recommendation Model Retraining

To automate the retraining of our cluster-based recommendation model, I will:

1. **Collect and preprocess** new song data and user listening history continuously.
2. **Set up** a weekly process to update the K-means clustering algorithm, recalculating user preferences based on their recent interactions with songs.
3. **Monitor** performance metrics such as user engagement and cluster cohesion, with alerts for any significant deviations.
4. **Automatically deploy** the updated model if it shows better performance in A/B tests, while having a rollback mechanism in case of issues.
5. **Integrate** user feedback to refine cluster assignments.
6. **Implement** drift detection to trigger a complete retraining when necessary.

By ensuring that our model stays up-to-date with evolving trends and preferences, we can improve the personalization of music recommendations, thereby increasing user satisfaction and engagement on our platform.

To automate the retraining of our cluster-based recommendation model, I will continuously collect and preprocess new song data and user listening history. I will set up a weekly process to update the K-means clustering algorithm, recalculating user preferences based on their recent interactions with songs. Performance metrics such as user engagement and cluster cohesion will be monitored, with alerts for any significant deviations. If the updated model shows better performance in A/B tests, it will be automatically deployed, while a rollback mechanism will be in place in case of issues. Additionally, I will integrate user feedback to refine cluster assignments and implement drift detection to trigger a complete retraining when necessary. By ensuring that our model stays up-to-date with evolving trends and preferences, we can improve the personalization of music recommendations, thereby increasing user satisfaction and engagement on our platform.